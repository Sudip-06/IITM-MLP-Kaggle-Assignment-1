{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":105665,"databundleVersionId":12770423,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **IITM DATA SCIENCE MLP Kaggle Assignment 1**\n\nIn this assignment my task is to predict the price of Flight tickets. ","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries and Datasets","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, OneHotEncoder,PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import r2_score\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import VotingRegressor\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:01.481342Z","iopub.execute_input":"2025-07-07T10:14:01.481645Z","iopub.status.idle":"2025-07-07T10:14:11.419982Z","shell.execute_reply.started":"2025-07-07T10:14:01.481615Z","shell.execute_reply":"2025-07-07T10:14:11.419116Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/mlp-term-2-2025-kaggle-assignment-1/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/mlp-term-2-2025-kaggle-assignment-1/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:11.420867Z","iopub.execute_input":"2025-07-07T10:14:11.421446Z","iopub.status.idle":"2025-07-07T10:14:11.609164Z","shell.execute_reply.started":"2025-07-07T10:14:11.421423Z","shell.execute_reply":"2025-07-07T10:14:11.608382Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Data Type Identification\n\nThe dataset contains the following columns with their respective data types:\n\n- `id`: *int64* (numerical, identifier)\n- `airline`: *object* (categorical)\n- `flight`: *object* (categorical, may have high cardinality)\n- `source`: *object* (categorical)\n- `departure`: *object* (categorical/time)\n- `stops`: *object* (categorical, can be converted to ordinal or integer)\n- `arrival`: *object* (categorical/time)\n- `destination`: *object* (categorical)\n- `class`: *object* (categorical)\n- `duration`: *float64* (numerical, duration of flight)\n- `days_left`: *float64* (numerical, derived feature)\n- `price`: *int64* (numerical, target variable)","metadata":{}},{"cell_type":"code","source":"train.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:11.610074Z","iopub.execute_input":"2025-07-07T10:14:11.610394Z","iopub.status.idle":"2025-07-07T10:14:11.623136Z","shell.execute_reply.started":"2025-07-07T10:14:11.610369Z","shell.execute_reply":"2025-07-07T10:14:11.622351Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"id               int64\nairline         object\nflight          object\nsource          object\ndeparture       object\nstops           object\narrival         object\ndestination     object\nclass           object\nduration       float64\ndays_left      float64\nprice            int64\ndtype: object"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Descriptive Statistics","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:11.625610Z","iopub.execute_input":"2025-07-07T10:14:11.625919Z","iopub.status.idle":"2025-07-07T10:14:11.674593Z","shell.execute_reply.started":"2025-07-07T10:14:11.625895Z","shell.execute_reply":"2025-07-07T10:14:11.673689Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                id      duration     days_left         price\ncount  40000.00000  36987.000000  35562.000000   40000.00000\nmean   19999.50000     12.004088     26.197936   20801.49025\nstd    11547.14972      7.108063     13.469232   22729.14842\nmin        0.00000      0.830000      1.000000    1105.00000\n25%     9999.75000      6.670000     15.000000    4687.00000\n50%    19999.50000     11.080000     26.000000    7353.00000\n75%    29999.25000     15.920000     38.000000   42521.00000\nmax    39999.00000     47.080000     49.000000  114704.00000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>duration</th>\n      <th>days_left</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>40000.00000</td>\n      <td>36987.000000</td>\n      <td>35562.000000</td>\n      <td>40000.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>19999.50000</td>\n      <td>12.004088</td>\n      <td>26.197936</td>\n      <td>20801.49025</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>11547.14972</td>\n      <td>7.108063</td>\n      <td>13.469232</td>\n      <td>22729.14842</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.00000</td>\n      <td>0.830000</td>\n      <td>1.000000</td>\n      <td>1105.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>9999.75000</td>\n      <td>6.670000</td>\n      <td>15.000000</td>\n      <td>4687.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>19999.50000</td>\n      <td>11.080000</td>\n      <td>26.000000</td>\n      <td>7353.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>29999.25000</td>\n      <td>15.920000</td>\n      <td>38.000000</td>\n      <td>42521.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>39999.00000</td>\n      <td>47.080000</td>\n      <td>49.000000</td>\n      <td>114704.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Missing Values Handling","metadata":{}},{"cell_type":"markdown","source":"Instead of dropping these rows, I decided to **impute them** to retain as much data as possible:\n\n- **Numerical columns** (like `duration`, `days_left`) were imputed using the **median**.\n- **Categorical columns** (like `airline`, `departure`, `stops`) were imputed using the **mode**.\n","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:11.677877Z","iopub.execute_input":"2025-07-07T10:14:11.678115Z","iopub.status.idle":"2025-07-07T10:14:11.704756Z","shell.execute_reply.started":"2025-07-07T10:14:11.678096Z","shell.execute_reply":"2025-07-07T10:14:11.703932Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"id                0\nairline        4613\nflight            0\nsource            0\ndeparture      4792\nstops          2319\narrival           0\ndestination       0\nclass             0\nduration       3013\ndays_left      4438\nprice             0\ndtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Duplicate Records Handling","metadata":{}},{"cell_type":"code","source":"train.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:11.705682Z","iopub.execute_input":"2025-07-07T10:14:11.706000Z","iopub.status.idle":"2025-07-07T10:14:11.746270Z","shell.execute_reply.started":"2025-07-07T10:14:11.705971Z","shell.execute_reply":"2025-07-07T10:14:11.745469Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"There are **no duplicate rows** in the dataset, so no action was required.","metadata":{}},{"cell_type":"markdown","source":"# import numpy as np\nimport pandas as pd\n\ndef mad_based_outlier_removal(df, cols, bounds=None):\n    if bounds is None:\n        bounds = {}\n        for col in cols:\n            median = df[col].median()\n            mad = np.median(np.abs(df[col] - median))\n            lower = median - 3 * mad\n            upper = median + 3 * mad\n            bounds[col] = (lower, upper)\n    for col in cols:\n        lower, upper = bounds[col]\n        df[col] = df[col].clip(lower=lower, upper=upper)\n    return df, bounds\n\n# Identify numeric columns\nnumeric_cols = [col for col in train.columns if train[col].dtype != 'object']\n\n# Calculate bounds on train, apply to both train and test\ntrain, bounds = mad_based_outlier_removal(train, numeric_cols)\ntest, _ = mad_based_outlier_removal(test, numeric_cols, bounds)\nOutlier Detection \n\nI used boxplots to visually inspect outliers in the numerical columns of the dataset.  \nWhile some outliers are present, I decided **not to remove them** because the models I trained are primarily **tree-based models** (like Random Forest, XGBoost, etc.), which are inherently robust to outliers.\n\nSo, retaining these values does not negatively affect model performance and helps preserve the natural distribution of the data.\n","metadata":{}},{"cell_type":"code","source":"for i in (train, test):\n    for col in i.columns:\n        if i[col].dtype != 'object':\n            Q1 = i[col].quantile(0.25)\n            Q3 = i[col].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - 1.5*IQR\n            upper_bound = Q3 + 1.5*IQR\n\n            i[col].clip(lower=lower_bound, upper=upper_bound, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:11.747244Z","iopub.execute_input":"2025-07-07T10:14:11.747567Z","iopub.status.idle":"2025-07-07T10:14:11.785018Z","shell.execute_reply.started":"2025-07-07T10:14:11.747540Z","shell.execute_reply":"2025-07-07T10:14:11.784215Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Remove id column","metadata":{}},{"cell_type":"code","source":"train = train.drop(columns = [\"id\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:11.785883Z","iopub.execute_input":"2025-07-07T10:14:11.786178Z","iopub.status.idle":"2025-07-07T10:14:11.798479Z","shell.execute_reply.started":"2025-07-07T10:14:11.786150Z","shell.execute_reply":"2025-07-07T10:14:11.797620Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"test = test.drop(columns = [\"id\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:11.799256Z","iopub.execute_input":"2025-07-07T10:14:11.799506Z","iopub.status.idle":"2025-07-07T10:14:11.814529Z","shell.execute_reply.started":"2025-07-07T10:14:11.799488Z","shell.execute_reply":"2025-07-07T10:14:11.813681Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"X_train = train.drop(columns=[\"price\"]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:11.815603Z","iopub.execute_input":"2025-07-07T10:14:11.815872Z","iopub.status.idle":"2025-07-07T10:14:11.828979Z","shell.execute_reply.started":"2025-07-07T10:14:11.815853Z","shell.execute_reply":"2025-07-07T10:14:11.828221Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Feature Encoding and Imputation","metadata":{}},{"cell_type":"code","source":"median_columns = [\"duration\", \"days_left\"]\nohe_columns = [\"airline\", \"source\", \"departure\", \"arrival\", \"destination\"]\nordinal_columns = [\"flight\", \"stops\", \"class\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:11.830306Z","iopub.execute_input":"2025-07-07T10:14:11.830643Z","iopub.status.idle":"2025-07-07T10:14:11.843007Z","shell.execute_reply.started":"2025-07-07T10:14:11.830615Z","shell.execute_reply":"2025-07-07T10:14:11.842035Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"column_transformer = ColumnTransformer(transformers=[\n    ('median_col',  SimpleImputer(strategy='median'), median_columns),\n\n    ('ohe_col', Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('encoder', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n    ]), ohe_columns),\n\n    ('ordinal_col', Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n    ]), ordinal_columns)\n])\n\nX_train_transformed = column_transformer.fit_transform(X_train)\ntest_transformed = column_transformer.transform(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:11.846731Z","iopub.execute_input":"2025-07-07T10:14:11.847448Z","iopub.status.idle":"2025-07-07T10:14:12.054475Z","shell.execute_reply.started":"2025-07-07T10:14:11.847414Z","shell.execute_reply":"2025-07-07T10:14:12.053619Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Feature Scaling\nSince we are using tree-based models (like Random Forest, XGBoost), feature scaling is not necessary as they are not sensitive to feature magnitudes.","metadata":{}},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"X = X_train_transformed\ny = train[\"price\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:12.055416Z","iopub.execute_input":"2025-07-07T10:14:12.055688Z","iopub.status.idle":"2025-07-07T10:14:12.060045Z","shell.execute_reply.started":"2025-07-07T10:14:12.055669Z","shell.execute_reply":"2025-07-07T10:14:12.059301Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Train-Test Split","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1437)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:12.060964Z","iopub.execute_input":"2025-07-07T10:14:12.061218Z","iopub.status.idle":"2025-07-07T10:14:12.089029Z","shell.execute_reply.started":"2025-07-07T10:14:12.061198Z","shell.execute_reply":"2025-07-07T10:14:12.088065Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Linear Regression\n\nMy **Linear Regression model** achieved an **RÂ² score of 0.90**, which **clearly crosses the required threshold of 0.80**.","metadata":{}},{"cell_type":"code","source":"regression_model = LinearRegression()\n\nregression_model.fit(X_train, y_train)\ny_pred = regression_model.predict(X_test)\n\nregression_r2_score = r2_score(y_test, y_pred)\nregression_r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:12.089950Z","iopub.execute_input":"2025-07-07T10:14:12.090221Z","iopub.status.idle":"2025-07-07T10:14:12.302214Z","shell.execute_reply.started":"2025-07-07T10:14:12.090201Z","shell.execute_reply":"2025-07-07T10:14:12.301440Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0.9077473123917048"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Ridge Regression\n\nThis model also achieved an **RÂ² score of 0.90**, which **crosses the required threshold of 0.80**, indicating strong predictive performance.","metadata":{}},{"cell_type":"code","source":"ridge_model = Ridge()\n\nridge_model.fit(X_train, y_train)\ny_pred = ridge_model.predict(X_test)\n\nridge_r2_score = r2_score(y_test, y_pred)\nridge_r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:12.302839Z","iopub.execute_input":"2025-07-07T10:14:12.303084Z","iopub.status.idle":"2025-07-07T10:14:12.388026Z","shell.execute_reply.started":"2025-07-07T10:14:12.303065Z","shell.execute_reply":"2025-07-07T10:14:12.387260Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0.9077462454303461"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Lasso Regression\nLasso Regression was applied, and it achieved RÂ² score of **0.90**.","metadata":{}},{"cell_type":"code","source":"lasso_model = Lasso()\n\nlasso_model.fit(X_train, y_train)\ny_pred = lasso_model.predict(X_test)\n\nlasso_r2_score = r2_score(y_test, y_pred)\nlasso_r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:12.389202Z","iopub.execute_input":"2025-07-07T10:14:12.389527Z","iopub.status.idle":"2025-07-07T10:14:12.497749Z","shell.execute_reply.started":"2025-07-07T10:14:12.389498Z","shell.execute_reply":"2025-07-07T10:14:12.497023Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0.9077376284473211"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"## Polynomial Regression (Degree 2)\nPolynomial Regression with degree 2 was applied to capture non-linear patterns in the data. The model performed well, achieving an RÂ² score of **0.94**","metadata":{}},{"cell_type":"code","source":"polynomial_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\npolynomial_model.fit(X_train, y_train)\n\ny_pred = polynomial_model.predict(X_test)\npoly_r2_score = r2_score(y_test, y_pred)\npoly_r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:12.498936Z","iopub.execute_input":"2025-07-07T10:14:12.499530Z","iopub.status.idle":"2025-07-07T10:14:15.190111Z","shell.execute_reply.started":"2025-07-07T10:14:12.499503Z","shell.execute_reply":"2025-07-07T10:14:15.187454Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0.9446493130366147"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## K-Nearest Neighbors Regression\nK-Nearest Neighbors (KNN) Regression was also applied. However, it performed relatively poorly compared to Linear, Lasso, and Ridge regressions. It achieved an RÂ² score of **0.29**","metadata":{}},{"cell_type":"code","source":"KNN_model = KNeighborsRegressor()\nKNN_model.fit(X_train, y_train)\n\ny_pred = KNN_model.predict(X_test)\nknn_r2_score = r2_score(y_test, y_pred)\nknn_r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:15.191552Z","iopub.execute_input":"2025-07-07T10:14:15.194053Z","iopub.status.idle":"2025-07-07T10:14:15.895841Z","shell.execute_reply.started":"2025-07-07T10:14:15.194020Z","shell.execute_reply":"2025-07-07T10:14:15.895349Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0.2906872653007082"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"## Decision Tree Regression\nDecision Tree Regression was applied  and delivered impressive performance. It achieved an RÂ² score of **0.96**, outperforming Linear, Lasso, Ridge, and KNN models by effectively capturing complex patterns in the data.","metadata":{}},{"cell_type":"code","source":"dt_model = DecisionTreeRegressor()\ndt_model.fit(X_train, y_train)\n\ny_pred = dt_model.predict(X_test)\ndt_r2_score = r2_score(y_test, y_pred)\ndt_r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:15.896416Z","iopub.execute_input":"2025-07-07T10:14:15.896699Z","iopub.status.idle":"2025-07-07T10:14:16.140003Z","shell.execute_reply.started":"2025-07-07T10:14:15.896673Z","shell.execute_reply":"2025-07-07T10:14:16.139343Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0.9634061929078703"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## Random Forest Regression\nThe Random Forest model leveraged the power of ensemble learning by combining multiple decision trees. It captured complex patterns in the data effectively and delivered a high RÂ² score of **0.97**, showcasing its strong predictive capability compared to other models.","metadata":{}},{"cell_type":"code","source":"rf_model = RandomForestRegressor()\nrf_model.fit(X_train, y_train)\n\ny_pred = rf_model.predict(X_test)\nrf_r2_score = r2_score(y_test, y_pred)\nrf_r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:16.140832Z","iopub.execute_input":"2025-07-07T10:14:16.141083Z","iopub.status.idle":"2025-07-07T10:14:31.854212Z","shell.execute_reply.started":"2025-07-07T10:14:16.141063Z","shell.execute_reply":"2025-07-07T10:14:31.853379Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"0.9792990904872252"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"## Gradient Boosting Regression\nGradient Boosting Regression achieved an RÂ² score of **0.95**, slightly lower than Random Forest but still showing strong predictive performance.","metadata":{}},{"cell_type":"code","source":"gboost_model = GradientBoostingRegressor()\ngboost_model.fit(X, y)\n\ny_pred = gboost_model.predict(X_test)\ngb_r2_score = r2_score(y_test, y_pred)\ngb_r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:31.855086Z","iopub.execute_input":"2025-07-07T10:14:31.855452Z","iopub.status.idle":"2025-07-07T10:14:37.666613Z","shell.execute_reply.started":"2025-07-07T10:14:31.855429Z","shell.execute_reply":"2025-07-07T10:14:37.665717Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"0.9552786617419292"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"##  XGBoost Regression\nXGBoost Regression delivered the  RÂ² score of **0.97** . Its powerful gradient boosting mechanism nailed the patterns in the data with precision.","metadata":{}},{"cell_type":"code","source":"xgb_model = XGBRegressor(random_state=42)\nxgb_model.fit(X_train, y_train)\n\ny_pred = xgb_model.predict(X_test)\nxgb_r2_score = r2_score(y_test, y_pred)\nxgb_r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:37.667522Z","iopub.execute_input":"2025-07-07T10:14:37.667956Z","iopub.status.idle":"2025-07-07T10:14:38.034647Z","shell.execute_reply.started":"2025-07-07T10:14:37.667935Z","shell.execute_reply":"2025-07-07T10:14:38.033204Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0.976568126433574"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## LightGBM Regression\nLast but not the least, LightGBM Regression achieved an impressive RÂ² score of **0.97**. Fast, efficient, and nearly on par with XGBoost â€” it proved to be a solid contender in the lineup.","metadata":{}},{"cell_type":"code","source":"lgb_model = LGBMRegressor(random_state=42)\nlgb_model.fit(X_train, y_train)\n\ny_pred = lgb_model.predict(X_test)\nlgb_r2_score = r2_score(y_test, y_pred)\nlgb_r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:38.035386Z","iopub.execute_input":"2025-07-07T10:14:38.035625Z","iopub.status.idle":"2025-07-07T10:14:38.367892Z","shell.execute_reply.started":"2025-07-07T10:14:38.035607Z","shell.execute_reply":"2025-07-07T10:14:38.367019Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004000 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 614\n[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 35\n[LightGBM] [Info] Start training from score 20762.366187\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"0.9730827677362757"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"# Hyperparameter Tuning & Model Selection","metadata":{}},{"cell_type":"markdown","source":"For hyperparameter tuning, I selected the top 3 performing models â€” **Random Forest**, **XGBoost**, and **LightGBM** â€” based on their high RÂ² scores and overall performance.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nimport optuna\n\n\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 1400, 1500),\n        'max_depth': trial.suggest_int('max_depth', 9, 9),\n        'learning_rate': trial.suggest_float('learning_rate', 0.015, 0.07),\n        'subsample': trial.suggest_float('subsample', 0.92, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.85, 0.98),\n        'reg_alpha': trial.suggest_float('reg_alpha', 4.0, 7.5),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1.4, 2.2),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 6),\n        'gamma': trial.suggest_float('gamma', 3.0, 3.6),\n        'tree_method': 'hist',     # Use hist + device=cuda for GPU (XGBoost >= 2.0)\n        'device': 'cuda',\n        'random_state': 42,\n        'n_jobs': -1\n    }\n    model = XGBRegressor(**params)\n    return cross_val_score(model, X, y, cv=5, scoring='r2').mean()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\nprint(\"âœ… Best RÂ² Score:\", study.best_value)\nprint(\"ðŸŽ¯ Best Hyperparameters:\", study.best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:15:20.518856Z","iopub.execute_input":"2025-07-07T10:15:20.519683Z"}},"outputs":[{"name":"stderr","text":"[I 2025-07-07 10:15:20,523] A new study created in memory with name: no-name-fbd7afc1-9faf-4405-9595-1895a6c84aa8\n[I 2025-07-07 10:15:58,279] Trial 0 finished with value: 0.9820034944381495 and parameters: {'n_estimators': 1422, 'max_depth': 9, 'learning_rate': 0.03758834900871876, 'subsample': 0.941574158745524, 'colsample_bytree': 0.9363018574222175, 'reg_alpha': 5.826046320266316, 'reg_lambda': 1.8726629265983334, 'min_child_weight': 5, 'gamma': 3.355874619203299}. Best is trial 0 with value: 0.9820034944381495.\n[I 2025-07-07 10:16:40,487] Trial 1 finished with value: 0.9819002889880437 and parameters: {'n_estimators': 1465, 'max_depth': 9, 'learning_rate': 0.04842001900344684, 'subsample': 0.9789106913258045, 'colsample_bytree': 0.9175965404271255, 'reg_alpha': 5.454157107710865, 'reg_lambda': 2.0037119393163785, 'min_child_weight': 1, 'gamma': 3.5446494016826082}. Best is trial 0 with value: 0.9820034944381495.\n[I 2025-07-07 10:17:16,384] Trial 2 finished with value: 0.9819489816342616 and parameters: {'n_estimators': 1492, 'max_depth': 9, 'learning_rate': 0.04057147569253823, 'subsample': 0.9981632294069246, 'colsample_bytree': 0.9575153267013012, 'reg_alpha': 4.173387907000932, 'reg_lambda': 1.8398906562403476, 'min_child_weight': 5, 'gamma': 3.444529922747592}. Best is trial 0 with value: 0.9820034944381495.\n[I 2025-07-07 10:17:54,686] Trial 3 finished with value: 0.9817582019246419 and parameters: {'n_estimators': 1473, 'max_depth': 9, 'learning_rate': 0.015271233165713582, 'subsample': 0.9798415650317308, 'colsample_bytree': 0.9519474526541463, 'reg_alpha': 5.392040764578902, 'reg_lambda': 1.5620150628255638, 'min_child_weight': 4, 'gamma': 3.248183918626596}. Best is trial 0 with value: 0.9820034944381495.\n[I 2025-07-07 10:18:34,151] Trial 4 finished with value: 0.9820027313217075 and parameters: {'n_estimators': 1498, 'max_depth': 9, 'learning_rate': 0.029427485271933056, 'subsample': 0.9378409748339204, 'colsample_bytree': 0.9012619067881921, 'reg_alpha': 4.77497898149358, 'reg_lambda': 1.669667107259595, 'min_child_weight': 6, 'gamma': 3.469579876662064}. Best is trial 0 with value: 0.9820034944381495.\n[I 2025-07-07 10:19:14,234] Trial 5 finished with value: 0.9820097868598218 and parameters: {'n_estimators': 1447, 'max_depth': 9, 'learning_rate': 0.04305138698344399, 'subsample': 0.9260121000336192, 'colsample_bytree': 0.8847491516373891, 'reg_alpha': 5.020273818151505, 'reg_lambda': 1.4513875055427325, 'min_child_weight': 3, 'gamma': 3.3988652670314656}. Best is trial 5 with value: 0.9820097868598218.\n[I 2025-07-07 10:19:56,905] Trial 6 finished with value: 0.9818681926372692 and parameters: {'n_estimators': 1407, 'max_depth': 9, 'learning_rate': 0.05526001452080801, 'subsample': 0.92710941324362, 'colsample_bytree': 0.8567016927973516, 'reg_alpha': 6.15932820446155, 'reg_lambda': 1.6618157585302922, 'min_child_weight': 1, 'gamma': 3.3719986399842408}. Best is trial 5 with value: 0.9820097868598218.\n[I 2025-07-07 10:20:38,805] Trial 7 finished with value: 0.9823056794426517 and parameters: {'n_estimators': 1439, 'max_depth': 9, 'learning_rate': 0.028080656996084637, 'subsample': 0.975399672452133, 'colsample_bytree': 0.9402548748949124, 'reg_alpha': 4.6275640111838845, 'reg_lambda': 2.0394632955012026, 'min_child_weight': 2, 'gamma': 3.065891932357042}. Best is trial 7 with value: 0.9823056794426517.\n[I 2025-07-07 10:21:16,355] Trial 8 finished with value: 0.9814586060362102 and parameters: {'n_estimators': 1407, 'max_depth': 9, 'learning_rate': 0.06489431688395175, 'subsample': 0.9691121887318662, 'colsample_bytree': 0.9736913097032576, 'reg_alpha': 4.276321986150235, 'reg_lambda': 1.695003989902403, 'min_child_weight': 5, 'gamma': 3.3331690748843386}. Best is trial 7 with value: 0.9823056794426517.\n[I 2025-07-07 10:21:59,857] Trial 9 finished with value: 0.9818608357655046 and parameters: {'n_estimators': 1481, 'max_depth': 9, 'learning_rate': 0.04644203157467428, 'subsample': 0.926260095773235, 'colsample_bytree': 0.9314165407684637, 'reg_alpha': 5.614972426608111, 'reg_lambda': 1.891372642207983, 'min_child_weight': 2, 'gamma': 3.0302208776622916}. Best is trial 7 with value: 0.9823056794426517.\n[I 2025-07-07 10:22:39,467] Trial 10 finished with value: 0.9822984921594134 and parameters: {'n_estimators': 1439, 'max_depth': 9, 'learning_rate': 0.022984241253471274, 'subsample': 0.9561568140244244, 'colsample_bytree': 0.9792648626442754, 'reg_alpha': 7.317959180996248, 'reg_lambda': 2.1761371814674244, 'min_child_weight': 3, 'gamma': 3.0328813792032596}. Best is trial 7 with value: 0.9823056794426517.\n[I 2025-07-07 10:23:20,858] Trial 11 finished with value: 0.982265458604069 and parameters: {'n_estimators': 1438, 'max_depth': 9, 'learning_rate': 0.02384850077221197, 'subsample': 0.955617038977761, 'colsample_bytree': 0.9774370529264016, 'reg_alpha': 7.341862254428971, 'reg_lambda': 2.1975647559543785, 'min_child_weight': 3, 'gamma': 3.0471046880939183}. Best is trial 7 with value: 0.9823056794426517.\n[I 2025-07-07 10:24:01,681] Trial 12 finished with value: 0.9823944057795518 and parameters: {'n_estimators': 1432, 'max_depth': 9, 'learning_rate': 0.026122613052924133, 'subsample': 0.9570123687956829, 'colsample_bytree': 0.9502376708223337, 'reg_alpha': 7.105586957703799, 'reg_lambda': 2.1711497213696394, 'min_child_weight': 2, 'gamma': 3.1703191576292156}. Best is trial 12 with value: 0.9823944057795518.\n[I 2025-07-07 10:24:41,457] Trial 13 finished with value: 0.9823431376506822 and parameters: {'n_estimators': 1425, 'max_depth': 9, 'learning_rate': 0.032200872020150526, 'subsample': 0.9693116089889475, 'colsample_bytree': 0.9464125451900217, 'reg_alpha': 6.598789146392929, 'reg_lambda': 2.0459573671253013, 'min_child_weight': 2, 'gamma': 3.1939981945225835}. Best is trial 12 with value: 0.9823944057795518.\n[I 2025-07-07 10:25:22,984] Trial 14 finished with value: 0.9823537308706701 and parameters: {'n_estimators': 1423, 'max_depth': 9, 'learning_rate': 0.03443880989095022, 'subsample': 0.9646727697217802, 'colsample_bytree': 0.9575499082657044, 'reg_alpha': 6.6511150962490575, 'reg_lambda': 2.0553070169882472, 'min_child_weight': 2, 'gamma': 3.20820064709252}. Best is trial 12 with value: 0.9823944057795518.\n[I 2025-07-07 10:26:06,805] Trial 15 finished with value: 0.9820694998354046 and parameters: {'n_estimators': 1423, 'max_depth': 9, 'learning_rate': 0.01659014823559079, 'subsample': 0.9477979693193666, 'colsample_bytree': 0.962762573242863, 'reg_alpha': 6.755506617590463, 'reg_lambda': 2.1091819270828642, 'min_child_weight': 1, 'gamma': 3.147936992311826}. Best is trial 12 with value: 0.9823944057795518.\n[I 2025-07-07 10:26:46,347] Trial 16 finished with value: 0.982187956077435 and parameters: {'n_estimators': 1460, 'max_depth': 9, 'learning_rate': 0.03393769660431586, 'subsample': 0.9901951369034103, 'colsample_bytree': 0.9226717426005437, 'reg_alpha': 6.783815095316295, 'reg_lambda': 1.9580399547755845, 'min_child_weight': 2, 'gamma': 3.1301565649197483}. Best is trial 12 with value: 0.9823944057795518.\n[I 2025-07-07 10:27:23,398] Trial 17 finished with value: 0.98185803349806 and parameters: {'n_estimators': 1400, 'max_depth': 9, 'learning_rate': 0.05370708524524249, 'subsample': 0.9617964687475056, 'colsample_bytree': 0.9026033326427304, 'reg_alpha': 6.3133123478806175, 'reg_lambda': 2.092943032140924, 'min_child_weight': 4, 'gamma': 3.2516749995286216}. Best is trial 12 with value: 0.9823944057795518.\n[I 2025-07-07 10:28:03,376] Trial 18 finished with value: 0.98234934575816 and parameters: {'n_estimators': 1429, 'max_depth': 9, 'learning_rate': 0.021887484063457516, 'subsample': 0.9507402672729824, 'colsample_bytree': 0.9624475781375376, 'reg_alpha': 7.048902457718506, 'reg_lambda': 1.94560659376415, 'min_child_weight': 2, 'gamma': 3.2710354496178824}. Best is trial 12 with value: 0.9823944057795518.\n[I 2025-07-07 10:28:48,433] Trial 19 finished with value: 0.9816474145639651 and parameters: {'n_estimators': 1453, 'max_depth': 9, 'learning_rate': 0.06708756754447055, 'subsample': 0.9646412006349463, 'colsample_bytree': 0.9020579085201683, 'reg_alpha': 6.273785159822942, 'reg_lambda': 1.7741042981293476, 'min_child_weight': 1, 'gamma': 3.184165040425999}. Best is trial 12 with value: 0.9823944057795518.\n[I 2025-07-07 10:29:27,016] Trial 20 finished with value: 0.9821672592186346 and parameters: {'n_estimators': 1416, 'max_depth': 9, 'learning_rate': 0.03673409379229191, 'subsample': 0.9385279785725951, 'colsample_bytree': 0.9293714740568945, 'reg_alpha': 7.48066610875285, 'reg_lambda': 2.1223840001097614, 'min_child_weight': 3, 'gamma': 3.0962322151006427}. Best is trial 12 with value: 0.9823944057795518.\n[I 2025-07-07 10:30:06,991] Trial 21 finished with value: 0.9823807612843293 and parameters: {'n_estimators': 1431, 'max_depth': 9, 'learning_rate': 0.02198496736313982, 'subsample': 0.9550113152860337, 'colsample_bytree': 0.9629082706037132, 'reg_alpha': 7.030002281167538, 'reg_lambda': 1.953132136488609, 'min_child_weight': 2, 'gamma': 3.271409304462299}. Best is trial 12 with value: 0.9823944057795518.\n[I 2025-07-07 10:30:47,789] Trial 22 finished with value: 0.9824001149566468 and parameters: {'n_estimators': 1432, 'max_depth': 9, 'learning_rate': 0.02743661492047145, 'subsample': 0.950674109720024, 'colsample_bytree': 0.9654159967596938, 'reg_alpha': 6.916001282083428, 'reg_lambda': 1.9629172021917558, 'min_child_weight': 2, 'gamma': 3.2192172311576805}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:31:28,145] Trial 23 finished with value: 0.9822471863915446 and parameters: {'n_estimators': 1434, 'max_depth': 9, 'learning_rate': 0.020196135205717232, 'subsample': 0.9479144772533015, 'colsample_bytree': 0.9654087347602336, 'reg_alpha': 7.035612310567629, 'reg_lambda': 1.779480158991166, 'min_child_weight': 3, 'gamma': 3.3047587649889745}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:32:10,540] Trial 24 finished with value: 0.9823627206661063 and parameters: {'n_estimators': 1454, 'max_depth': 9, 'learning_rate': 0.025772065910409703, 'subsample': 0.9551232161861255, 'colsample_bytree': 0.9464312201151612, 'reg_alpha': 7.0569991262123875, 'reg_lambda': 1.912841153103247, 'min_child_weight': 1, 'gamma': 3.1503260901144077}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:32:51,598] Trial 25 finished with value: 0.9822336985963028 and parameters: {'n_estimators': 1445, 'max_depth': 9, 'learning_rate': 0.018599461629307715, 'subsample': 0.944544745808032, 'colsample_bytree': 0.9679568388857468, 'reg_alpha': 6.072222122298708, 'reg_lambda': 1.98728705624452, 'min_child_weight': 2, 'gamma': 3.242020664513573}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:33:30,185] Trial 26 finished with value: 0.9822007268698346 and parameters: {'n_estimators': 1431, 'max_depth': 9, 'learning_rate': 0.027537021638519253, 'subsample': 0.9340949249981025, 'colsample_bytree': 0.9488992178482079, 'reg_alpha': 6.461127350152045, 'reg_lambda': 2.1469607904177104, 'min_child_weight': 4, 'gamma': 3.290622070984318}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:34:12,226] Trial 27 finished with value: 0.9823702745462848 and parameters: {'n_estimators': 1412, 'max_depth': 9, 'learning_rate': 0.030406072455679077, 'subsample': 0.9518735979668747, 'colsample_bytree': 0.9697686276464906, 'reg_alpha': 6.930206528672899, 'reg_lambda': 1.838279663215635, 'min_child_weight': 2, 'gamma': 3.118135707564463}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:34:50,069] Trial 28 finished with value: 0.9821233953708441 and parameters: {'n_estimators': 1418, 'max_depth': 9, 'learning_rate': 0.02017099074986362, 'subsample': 0.9590713951673505, 'colsample_bytree': 0.8532877776871376, 'reg_alpha': 7.237884827081328, 'reg_lambda': 1.9499267019075182, 'min_child_weight': 3, 'gamma': 3.1979553494996984}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:35:33,657] Trial 29 finished with value: 0.9820569860448456 and parameters: {'n_estimators': 1445, 'max_depth': 9, 'learning_rate': 0.040510859744108424, 'subsample': 0.9429442975898888, 'colsample_bytree': 0.9392265042132681, 'reg_alpha': 6.018656105070011, 'reg_lambda': 1.7407541779089457, 'min_child_weight': 1, 'gamma': 3.3267232473917807}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:36:10,507] Trial 30 finished with value: 0.9819774328508444 and parameters: {'n_estimators': 1429, 'max_depth': 9, 'learning_rate': 0.025427143607637762, 'subsample': 0.9334617805788588, 'colsample_bytree': 0.9564000147657975, 'reg_alpha': 5.832383437932563, 'reg_lambda': 1.8390245510754841, 'min_child_weight': 6, 'gamma': 3.3772789579102627}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:36:53,213] Trial 31 finished with value: 0.9823634431629815 and parameters: {'n_estimators': 1414, 'max_depth': 9, 'learning_rate': 0.03128933261245507, 'subsample': 0.9513466413455962, 'colsample_bytree': 0.9689464467302376, 'reg_alpha': 6.886850654199356, 'reg_lambda': 1.8560651129663897, 'min_child_weight': 2, 'gamma': 3.106978631648763}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:37:34,313] Trial 32 finished with value: 0.9822234470893964 and parameters: {'n_estimators': 1411, 'max_depth': 9, 'learning_rate': 0.03651004618057031, 'subsample': 0.9523167453404211, 'colsample_bytree': 0.9716152709673503, 'reg_alpha': 6.467803910133079, 'reg_lambda': 2.014669990993984, 'min_child_weight': 2, 'gamma': 3.0897318256903827}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:38:17,774] Trial 33 finished with value: 0.9823931859476005 and parameters: {'n_estimators': 1421, 'max_depth': 9, 'learning_rate': 0.030519750821593156, 'subsample': 0.9596423489374609, 'colsample_bytree': 0.957518731779202, 'reg_alpha': 6.9670923868351045, 'reg_lambda': 1.9029023923928392, 'min_child_weight': 1, 'gamma': 3.0036467146116337}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:39:01,812] Trial 34 finished with value: 0.9823844657910538 and parameters: {'n_estimators': 1435, 'max_depth': 9, 'learning_rate': 0.025423579437903132, 'subsample': 0.9696375094395004, 'colsample_bytree': 0.9514807001785885, 'reg_alpha': 7.214100001288694, 'reg_lambda': 1.9085699284993722, 'min_child_weight': 1, 'gamma': 3.2245549134320446}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:39:43,491] Trial 35 finished with value: 0.9821817690838671 and parameters: {'n_estimators': 1437, 'max_depth': 9, 'learning_rate': 0.03950628833215948, 'subsample': 0.9742967031681804, 'colsample_bytree': 0.9415068812610988, 'reg_alpha': 7.471255689543428, 'reg_lambda': 1.8756425372201055, 'min_child_weight': 1, 'gamma': 3.539666850474277}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:40:25,683] Trial 36 finished with value: 0.9823315821045391 and parameters: {'n_estimators': 1443, 'max_depth': 9, 'learning_rate': 0.0272794417237296, 'subsample': 0.9842671726809705, 'colsample_bytree': 0.9537372532580689, 'reg_alpha': 7.235366399291613, 'reg_lambda': 1.6087398242449102, 'min_child_weight': 1, 'gamma': 3.2280957011909086}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:41:10,378] Trial 37 finished with value: 0.9820748220476728 and parameters: {'n_estimators': 1462, 'max_depth': 9, 'learning_rate': 0.015362356143826254, 'subsample': 0.9679095869593792, 'colsample_bytree': 0.9108309316487649, 'reg_alpha': 7.198789747674317, 'reg_lambda': 1.4139711136150668, 'min_child_weight': 1, 'gamma': 3.002621176076652}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:41:52,439] Trial 38 finished with value: 0.9821486432183842 and parameters: {'n_estimators': 1421, 'max_depth': 9, 'learning_rate': 0.044775017050717986, 'subsample': 0.9607888201678524, 'colsample_bytree': 0.8847236726982977, 'reg_alpha': 5.13428896206487, 'reg_lambda': 1.8064378034349526, 'min_child_weight': 1, 'gamma': 3.432129809511109}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:42:32,232] Trial 39 finished with value: 0.9822508595162794 and parameters: {'n_estimators': 1400, 'max_depth': 9, 'learning_rate': 0.028954152674491865, 'subsample': 0.9867671130458746, 'colsample_bytree': 0.9264022800149309, 'reg_alpha': 6.587747597197021, 'reg_lambda': 2.0793391831044854, 'min_child_weight': 1, 'gamma': 3.155654468784721}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:43:09,434] Trial 40 finished with value: 0.9817876802916194 and parameters: {'n_estimators': 1469, 'max_depth': 9, 'learning_rate': 0.05176156819108611, 'subsample': 0.976680864284211, 'colsample_bytree': 0.9351190613556457, 'reg_alpha': 6.826977075605074, 'reg_lambda': 1.9997649648941245, 'min_child_weight': 5, 'gamma': 3.0013403419298514}. Best is trial 22 with value: 0.9824001149566468.\n[I 2025-07-07 10:43:50,687] Trial 41 finished with value: 0.982412091044807 and parameters: {'n_estimators': 1427, 'max_depth': 9, 'learning_rate': 0.022874537329734865, 'subsample': 0.971518035496103, 'colsample_bytree': 0.9589415633085722, 'reg_alpha': 7.1139620653760485, 'reg_lambda': 1.9289654847498579, 'min_child_weight': 2, 'gamma': 3.2850524885283}. Best is trial 41 with value: 0.982412091044807.\n[I 2025-07-07 10:44:32,300] Trial 42 finished with value: 0.9823662473254103 and parameters: {'n_estimators': 1426, 'max_depth': 9, 'learning_rate': 0.02495051800288541, 'subsample': 0.9735626760307666, 'colsample_bytree': 0.9553105834820449, 'reg_alpha': 7.13666396359304, 'reg_lambda': 1.9233239345828599, 'min_child_weight': 1, 'gamma': 3.334952600904679}. Best is trial 41 with value: 0.982412091044807.\n[I 2025-07-07 10:45:12,044] Trial 43 finished with value: 0.9821241121777711 and parameters: {'n_estimators': 1435, 'max_depth': 9, 'learning_rate': 0.018552989243053347, 'subsample': 0.9808792614289055, 'colsample_bytree': 0.947265932123944, 'reg_alpha': 7.4933927932080655, 'reg_lambda': 1.8983090520759132, 'min_child_weight': 2, 'gamma': 3.2249037077138953}. Best is trial 41 with value: 0.982412091044807.\n[I 2025-07-07 10:45:50,270] Trial 44 finished with value: 0.9822158874695465 and parameters: {'n_estimators': 1421, 'max_depth': 9, 'learning_rate': 0.03260100806768287, 'subsample': 0.9711906175298739, 'colsample_bytree': 0.9594076312964965, 'reg_alpha': 7.321691508867105, 'reg_lambda': 1.9817854721404502, 'min_child_weight': 3, 'gamma': 3.1686503303567926}. Best is trial 41 with value: 0.982412091044807.\n[I 2025-07-07 10:46:35,107] Trial 45 finished with value: 0.9824144687129547 and parameters: {'n_estimators': 1451, 'max_depth': 9, 'learning_rate': 0.029777699649083972, 'subsample': 0.9643379850571834, 'colsample_bytree': 0.9756657321300953, 'reg_alpha': 6.897452373324194, 'reg_lambda': 1.803070412157061, 'min_child_weight': 1, 'gamma': 3.0721599807917395}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:47:17,826] Trial 46 finished with value: 0.9817065394674931 and parameters: {'n_estimators': 1451, 'max_depth': 9, 'learning_rate': 0.059554710283402645, 'subsample': 0.9653670511509169, 'colsample_bytree': 0.9778974514313956, 'reg_alpha': 6.727102936743235, 'reg_lambda': 1.7203908548636693, 'min_child_weight': 2, 'gamma': 3.06830266547702}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:48:02,514] Trial 47 finished with value: 0.9823434148687928 and parameters: {'n_estimators': 1477, 'max_depth': 9, 'learning_rate': 0.029833515477467102, 'subsample': 0.959624163117041, 'colsample_bytree': 0.9738441958087091, 'reg_alpha': 6.45135242145025, 'reg_lambda': 1.5480795229330566, 'min_child_weight': 1, 'gamma': 3.0512294436665197}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:48:42,124] Trial 48 finished with value: 0.9820944859138312 and parameters: {'n_estimators': 1488, 'max_depth': 9, 'learning_rate': 0.03558033936246825, 'subsample': 0.9998591228758856, 'colsample_bytree': 0.8823599836668409, 'reg_alpha': 6.911175692545227, 'reg_lambda': 1.8234769100884987, 'min_child_weight': 2, 'gamma': 3.078336702123584}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:49:20,643] Trial 49 finished with value: 0.9822947893717769 and parameters: {'n_estimators': 1442, 'max_depth': 9, 'learning_rate': 0.023181578293855958, 'subsample': 0.9468329689483979, 'colsample_bytree': 0.8622367897343125, 'reg_alpha': 5.391229935106901, 'reg_lambda': 1.6553438405125174, 'min_child_weight': 3, 'gamma': 3.022323250810297}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:50:02,882] Trial 50 finished with value: 0.9821341365337577 and parameters: {'n_estimators': 1457, 'max_depth': 9, 'learning_rate': 0.03811634579341655, 'subsample': 0.9622400326781847, 'colsample_bytree': 0.9795616912039634, 'reg_alpha': 6.620277826945829, 'reg_lambda': 2.0297212133806983, 'min_child_weight': 2, 'gamma': 3.133722003424607}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:50:48,966] Trial 51 finished with value: 0.9823946944915287 and parameters: {'n_estimators': 1427, 'max_depth': 9, 'learning_rate': 0.026769140381702347, 'subsample': 0.9673996986168824, 'colsample_bytree': 0.9511544878837641, 'reg_alpha': 7.342813915904406, 'reg_lambda': 1.8655228234770243, 'min_child_weight': 1, 'gamma': 3.211640409792802}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:51:31,103] Trial 52 finished with value: 0.9823515234213975 and parameters: {'n_estimators': 1407, 'max_depth': 9, 'learning_rate': 0.02759459842143046, 'subsample': 0.9576733726666067, 'colsample_bytree': 0.94203985021878, 'reg_alpha': 7.366951133156569, 'reg_lambda': 1.773573639246813, 'min_child_weight': 1, 'gamma': 3.2682604806105413}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:52:13,731] Trial 53 finished with value: 0.9823657889426037 and parameters: {'n_estimators': 1428, 'max_depth': 9, 'learning_rate': 0.0327362848892884, 'subsample': 0.9659741832411857, 'colsample_bytree': 0.9640342933796471, 'reg_alpha': 6.965965994200174, 'reg_lambda': 1.8606348967409723, 'min_child_weight': 1, 'gamma': 3.177298785212709}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:52:58,442] Trial 54 finished with value: 0.9823141187110893 and parameters: {'n_estimators': 1440, 'max_depth': 9, 'learning_rate': 0.02109583121685409, 'subsample': 0.9720322911492599, 'colsample_bytree': 0.9581832756558795, 'reg_alpha': 7.147809043915086, 'reg_lambda': 1.8803967968180797, 'min_child_weight': 1, 'gamma': 3.3054424329580057}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:53:40,798] Trial 55 finished with value: 0.9824027359917673 and parameters: {'n_estimators': 1448, 'max_depth': 9, 'learning_rate': 0.023248501460614013, 'subsample': 0.9639153380212016, 'colsample_bytree': 0.9748332752720099, 'reg_alpha': 6.840170431076504, 'reg_lambda': 1.8122135014581056, 'min_child_weight': 2, 'gamma': 3.208771456329902}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:54:20,591] Trial 56 finished with value: 0.9821118667651142 and parameters: {'n_estimators': 1456, 'max_depth': 9, 'learning_rate': 0.018285738720461082, 'subsample': 0.9780744448636635, 'colsample_bytree': 0.9740962417686094, 'reg_alpha': 4.4613367020066, 'reg_lambda': 1.748759455174465, 'min_child_weight': 3, 'gamma': 3.217379144326571}. Best is trial 45 with value: 0.9824144687129547.\n[I 2025-07-07 10:55:00,783] Trial 57 finished with value: 0.9824161447438803 and parameters: {'n_estimators': 1448, 'max_depth': 9, 'learning_rate': 0.023251872427358454, 'subsample': 0.962974049636557, 'colsample_bytree': 0.9661679319370026, 'reg_alpha': 7.376247401096565, 'reg_lambda': 1.8052457627599356, 'min_child_weight': 2, 'gamma': 3.2488858563534877}. Best is trial 57 with value: 0.9824161447438803.\n[I 2025-07-07 10:55:41,975] Trial 58 finished with value: 0.9821742983075314 and parameters: {'n_estimators': 1448, 'max_depth': 9, 'learning_rate': 0.02389846068724958, 'subsample': 0.9938931402574125, 'colsample_bytree': 0.966698527767985, 'reg_alpha': 7.357759017803049, 'reg_lambda': 1.7007127068622443, 'min_child_weight': 2, 'gamma': 3.2454844337236493}. Best is trial 57 with value: 0.9824161447438803.\n[I 2025-07-07 10:56:23,729] Trial 59 finished with value: 0.9824148524085097 and parameters: {'n_estimators': 1467, 'max_depth': 9, 'learning_rate': 0.022887589625369804, 'subsample': 0.9639683668241099, 'colsample_bytree': 0.9751836583586395, 'reg_alpha': 6.756891767578618, 'reg_lambda': 1.8051876542327125, 'min_child_weight': 2, 'gamma': 3.291328204366769}. Best is trial 57 with value: 0.9824161447438803.\n[I 2025-07-07 10:57:04,355] Trial 60 finished with value: 0.9821690274682261 and parameters: {'n_estimators': 1470, 'max_depth': 9, 'learning_rate': 0.017904144055789453, 'subsample': 0.9217757121311372, 'colsample_bytree': 0.9747441208095312, 'reg_alpha': 6.748412647334233, 'reg_lambda': 1.8068138360988775, 'min_child_weight': 3, 'gamma': 3.354775479785249}. Best is trial 57 with value: 0.9824161447438803.\n[I 2025-07-07 10:57:47,681] Trial 61 finished with value: 0.982405037068949 and parameters: {'n_estimators': 1464, 'max_depth': 9, 'learning_rate': 0.022348326676244205, 'subsample': 0.9632201491910628, 'colsample_bytree': 0.9718124779336204, 'reg_alpha': 7.1015999692742495, 'reg_lambda': 1.781425188947705, 'min_child_weight': 2, 'gamma': 3.278830843327287}. Best is trial 57 with value: 0.9824161447438803.\n[I 2025-07-07 10:58:28,618] Trial 62 finished with value: 0.9824185921151992 and parameters: {'n_estimators': 1464, 'max_depth': 9, 'learning_rate': 0.022794002012692403, 'subsample': 0.9632931118455071, 'colsample_bytree': 0.9792625993364977, 'reg_alpha': 7.08321755334993, 'reg_lambda': 1.7691156101230552, 'min_child_weight': 2, 'gamma': 3.2769599223068093}. Best is trial 62 with value: 0.9824185921151992.\n[I 2025-07-07 10:59:09,638] Trial 63 finished with value: 0.9824273780662438 and parameters: {'n_estimators': 1467, 'max_depth': 9, 'learning_rate': 0.022027808008884476, 'subsample': 0.9625411594238498, 'colsample_bytree': 0.9798085569335232, 'reg_alpha': 7.105215570256862, 'reg_lambda': 1.7503963785186758, 'min_child_weight': 2, 'gamma': 3.287709331011665}. Best is trial 63 with value: 0.9824273780662438.\n[I 2025-07-07 10:59:50,741] Trial 64 finished with value: 0.9823388926449544 and parameters: {'n_estimators': 1466, 'max_depth': 9, 'learning_rate': 0.020459369382888535, 'subsample': 0.963168597137807, 'colsample_bytree': 0.9790239795504104, 'reg_alpha': 7.053798525618828, 'reg_lambda': 1.6639694102331484, 'min_child_weight': 2, 'gamma': 3.317406929989679}. Best is trial 63 with value: 0.9824273780662438.\n[I 2025-07-07 11:00:31,342] Trial 65 finished with value: 0.98229593727377 and parameters: {'n_estimators': 1479, 'max_depth': 9, 'learning_rate': 0.02189655857821283, 'subsample': 0.9704734266992244, 'colsample_bytree': 0.9706134932895714, 'reg_alpha': 7.134453703500903, 'reg_lambda': 1.7512421998188332, 'min_child_weight': 3, 'gamma': 3.286212422295777}. Best is trial 63 with value: 0.9824273780662438.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Model Selection \nAfter comparing the RÂ² scores of all three tuned models â€” Random Forest (**0.9793**), LightGBM (**0.9789**), and XGBoost (**0.9812**) â€” **XGBoost** emerged as the top performer. Therefore, it was selected as the final model for training and prediction.\n","metadata":{}},{"cell_type":"code","source":"#RANDOM FOREST REGRESSION\nrf_model = RandomForestRegressor(n_estimators=700,max_depth=24,min_samples_split=4,min_samples_leaf=1,max_features=None,random_state=42,n_jobs=-1)\nrf_model.fit(X_train, y_train)\ny_pred_val = rf_model.predict(X_test)\nrf_r2_score = r2_score(y_test, y_pred_val)\n\n#XGBOOT REGRESSION\nxgb_model = XGBRegressor(\n    n_estimators=1435,\n    max_depth=9,\n    learning_rate=0.02696991017898118,\n    subsample=0.9545380004264244,\n    colsample_bytree=0.8954209312950507,\n    reg_alpha=5.532947360465362,\n    reg_lambda=1.906818391216358,\n    min_child_weight=2,\n    gamma=3.0671639571913594,\n    objective='reg:squarederror',\n    random_state=42,\n    n_jobs=-1\n)\n\n\nxgb_model.fit(X_train, y_train)\ny_pred_val = xgb_model.predict(X_test)\nxgb_r2_score = r2_score(y_test, y_pred_val)\n\n#LIGHTGBM REGRESSION\nlgb_model = LGBMRegressor(n_estimators=1000,max_depth=15,learning_rate=0.17236,subsample=0.7715,colsample_bytree=0.9248,reg_alpha=8.9567,reg_lambda=1.5674,min_child_weight=1,min_split_gain=0.4894,random_state=42,n_jobs=-1,verbose=-1)\nlgb_model.fit(X_train, y_train)\ny_pred_val = lgb_model.predict(X_test)\nlgb_r2_score = r2_score(y_test, y_pred_val)\n\nprint(f\"Tuned Random Forest RÂ² Score: {rf_r2_score:.4f}\")\nprint(f\"Tuned XGBoost RÂ² Score: {xgb_r2_score:.4f}\")\nprint(f\"Tuned LightGBM RÂ² Score: {lgb_r2_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:38.565410Z","iopub.status.idle":"2025-07-07T10:14:38.565683Z","shell.execute_reply.started":"2025-07-07T10:14:38.565553Z","shell.execute_reply":"2025-07-07T10:14:38.565567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Model and Submission\nNow, I am using 100% of the training data to fit the final model.","metadata":{}},{"cell_type":"code","source":"xgb_model = XGBRegressor(\n    n_estimators=1431,\n    max_depth=9,\n    learning_rate=0.02682794029820691,\n    subsample=0.9534150710821826,\n    colsample_bytree=0.9042762617787571,\n    reg_alpha=6.403811137420917,\n    reg_lambda=2.0333285857846137,\n    min_child_weight=2,\n    gamma=3.2142404785425622,\n    objective='reg:squarederror',\n    random_state=42,\n    n_jobs=-1\n)\n\nxgb_model.fit(X, y)\n\n\ny_pred = xgb_model.predict(test_transformed)\nsubmission_df = pd.DataFrame({\n    'id': range(10000),\n    'price': y_pred\n})\n\n\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:14:38.566637Z","iopub.status.idle":"2025-07-07T10:14:38.566905Z","shell.execute_reply.started":"2025-07-07T10:14:38.566777Z","shell.execute_reply":"2025-07-07T10:14:38.566792Z"}},"outputs":[],"execution_count":null}]}